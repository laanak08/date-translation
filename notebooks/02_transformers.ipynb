{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "Transformers are an alternative to RNN architectures. The authors state that transformers have higher performance and can be trained faster, since they can be easily parallelized.\n",
    "\n",
    "Here we will use the Transformer architecture to classify dates to the rule they were generated from. While this is not a sequence to sequence task, we will still use the transformer to embed a sequence of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import, load, and define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# change this to run on a specific GPU on your machine, indexing starts at 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import string\n",
    "from pathlib import Path\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from fastai.basic_data import DataBunch, DatasetType\n",
    "from fastai.metrics import accuracy\n",
    "from fastai.train import Learner\n",
    "from fastprogress import progress_bar\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [6, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DATA_PATH = Path('../data')\n",
    "_RAW_PATH = _DATA_PATH / 'raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframes(raw_path):\n",
    "    df_names = ['train', 'valid', 'test']\n",
    "    dfs = [pd.read_csv(raw_path/f'{df_name}.csv') for df_name in df_names]\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def create_vocab():\n",
    "    tokens = ['<pad>', '<unk>', '<sos>', '<eos>'] + list(string.printable)\n",
    "    tok_to_int = {c: i for i, c in enumerate(tokens)}\n",
    "    int_to_tok = [c for c, i in tok_to_int.items()]\n",
    "    assert tok_to_int['<pad>'] == 0\n",
    "    assert tok_to_int['<unk>'] == 1\n",
    "    assert tok_to_int['<sos>'] == 2\n",
    "    return tok_to_int, int_to_tok\n",
    "\n",
    "def build_text_mapper(tok_to_int: dict) -> Callable:\n",
    "    def mapper(text: str):\n",
    "        return [tok_to_int['<sos>']] + [tok_to_int.get(c, 1) for c in text] + [tok_to_int['<eos>']]\n",
    "    return mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and create character -> int mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df, test_df = load_dataframes(_RAW_PATH)\n",
    "tok_to_int, int_to_tok = create_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a classification dataset\n",
    "Input: \n",
    "\n",
    "    encoder input: Noisy date text, i.e. `Saturday December 23, 1834`\n",
    "    \n",
    "output: \n",
    "\n",
    "    The integer corresponding to the date format which generated the encoder input.\n",
    "    \n",
    "The following dataset code is specific to pytorch and may seem a bit convoluted. It just maps an integer index to a single training sample, as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model will see sequences of length 32.\n",
    "# any date shorter than 32 characters will be padded.\n",
    "_MAXLEN = 32\n",
    "\n",
    "class DateDataset(utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Maps an index to a tuple of (inputs, outputs).\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    ds = DateDataset(df, text_mapper)\n",
    "    ds[0]\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, text_mapper: Callable) -> None:\n",
    "        self.df = df\n",
    "        self.x_col = 'input'\n",
    "        self.y_col = 'format'\n",
    "        self.maxlen = _MAXLEN\n",
    "        self.pad_val = 0\n",
    "        self.text_mapper = text_mapper\n",
    "        self.c = len(self.df[self.y_col].unique())  # num unique classes\n",
    "        \n",
    "        self._prepare_data()\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        self.df['encode_text'] = self.df[self.x_col].apply(self.get_tokens, args=('pre',))\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        sample = self.df.iloc[idx]\n",
    "        encode_text = sample.encode_text\n",
    "        format_label = torch.LongTensor([sample[self.y_col]])\n",
    "        return {\n",
    "            'encode_text': encode_text,\n",
    "            'output': format_label\n",
    "        }\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def get_tokens(self, text: str, mode: str) -> torch.LongTensor:\n",
    "        if random.random() < 0.5:\n",
    "            text = text.lower()\n",
    "        tokens = self.text_mapper(text)\n",
    "        tokens = self._pad_sequence(tokens, mode)\n",
    "        return torch.LongTensor(tokens)\n",
    "    \n",
    "    def _pad_sequence(self, seq: List, mode: str) -> List:\n",
    "        diff = self.maxlen - len(seq)\n",
    "        if diff < 0:\n",
    "            raise ValueError('Can not pad a sequence longer than maxlen')\n",
    "        if mode == 'pre':\n",
    "            return [self.pad_val] * diff + seq\n",
    "        elif mode == 'post':\n",
    "            return seq + [self.pad_val] * diff\n",
    "        else:\n",
    "            raise ValueError('mode must be pre or post')\n",
    "\n",
    "\n",
    "def collate_batch(batch: List):\n",
    "    \"\"\"Stack each sample together to make a batch.\"\"\"\n",
    "    # text will have shape (B, T)\n",
    "    encode_text = torch.stack([sample['encode_text'] for sample in batch])\n",
    "    # y, needs to be flat to calculate the loss\n",
    "    # y shape = (B*T, )\n",
    "    y = torch.cat([sample['output'] for sample in batch])\n",
    "    return encode_text, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mapper = build_text_mapper(tok_to_int)\n",
    "\n",
    "train_ds = DateDataset(train_df, text_mapper)\n",
    "valid_ds = DateDataset(valid_df, text_mapper)\n",
    "test_ds = DateDataset(test_df, text_mapper)\n",
    "\n",
    "data = DataBunch.create(train_ds, valid_ds, test_ds, bs=256, collate_fn=collate_batch, device=torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_batch\n",
      "tensor([[ 0,  0,  0,  ...,  8, 12,  3],\n",
      "        [ 0,  0,  0,  ...,  5, 10,  3],\n",
      "        [ 0,  0,  0,  ...,  4, 13,  3],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ..., 10,  5,  3],\n",
      "        [ 0,  0,  0,  ...,  4,  9,  3],\n",
      "        [ 0,  0,  0,  ..., 11, 12,  3]])\n",
      "Y_batch\n",
      "tensor([ 6, 10,  8,  6,  4,  2,  8,  8,  2,  6,  5, 11,  6, 12,  8,  6,  6,  7,\n",
      "         1,  1,  7,  6,  4,  6, 10,  6,  0,  8,  7,  8,  2,  7,  3,  3,  2,  8,\n",
      "         0,  2,  3,  3,  9, 12,  2, 11,  3,  8,  6,  6,  7,  0, 12, 12, 10,  2,\n",
      "         1,  9, 12, 11, 11, 11, 10,  2,  4, 12,  9, 12, 11,  9,  4, 11,  3,  7,\n",
      "         5,  0,  4, 12, 10,  3,  8,  3, 12,  8, 11,  0,  2, 12,  3,  7,  0,  5,\n",
      "        12,  2,  6,  6,  1,  3, 11,  5,  2,  4,  4,  3,  3,  2,  7,  0,  5,  2,\n",
      "         3,  4,  9,  6,  4,  5, 11, 10, 11,  1,  7, 11, 12, 11,  5,  5,  2,  4,\n",
      "         9,  3, 10,  3,  8,  5, 10,  0,  2,  1, 12,  1, 11, 11,  1,  5,  9,  5,\n",
      "         2,  1,  1,  9, 11,  2,  7,  2,  1, 12,  8,  0,  4,  3,  6,  2,  2,  4,\n",
      "         9,  2,  9, 11,  2,  6,  7, 11,  7, 10,  0,  4,  6,  4,  4,  6,  7,  6,\n",
      "        10, 10, 12, 10,  4, 11,  7,  0,  1,  0, 10,  5,  3,  8,  0,  8,  6, 10,\n",
      "         3,  4, 10,  4,  2,  2,  0, 11,  7,  3,  8,  6,  4,  5,  3, 11,  1,  5,\n",
      "         7,  7, 10,  1,  3,  0,  8,  0,  1,  4, 11,  3, 10,  7,  6, 11, 10,  8,\n",
      "         7,  0,  4,  4,  1,  0, 12,  4,  1, 10,  3,  0, 12,  2, 10,  6, 10,  6,\n",
      "         9,  5,  1,  9])\n",
      "Num classes: 13\n"
     ]
    }
   ],
   "source": [
    "# visualize one batch\n",
    "xb, yb = data.one_batch()\n",
    "print('X_batch')\n",
    "print(xb)\n",
    "print('Y_batch')\n",
    "print(yb)\n",
    "print('Num classes:', data.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Transformer Encoder Architecture\n",
    "\n",
    "We will use pytorch for this implementation. It is worth noting that pytorch has an implementation of the transformer architecture, both encoder and decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Potitional Embedding\n",
    "\n",
    "The positional embedding is critical for the transformer to be able to \"know\" where is the sequence it is looking at.\n",
    "\n",
    "Vaswani et al. used a sinusoid function to generate these embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# set model config\n",
    "# this is a pretty small network, any bigger and the model can easy solve this problem.\n",
    "# I am purposefully trying to make it difficult for the vanilla model to \"solve\" this problem.\n",
    "\n",
    "_HIDDEN_SIZE = 64\n",
    "_NUM_LAYERS = 2\n",
    "_NUM_HEADS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, maxlen: int, hidden_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "            maxlen,\n",
    "            hidden_size\n",
    "        )\n",
    "        self._init_position_weights()\n",
    "\n",
    "    def _init_position_weights(self) -> None:\n",
    "        \"\"\"\n",
    "        Set and freeze the sinusoid weights.\n",
    "        Attention is all you need - Section 3.5\n",
    "        \"\"\"\n",
    "        W = torch.zeros_like(self.embedding_layer.weight)\n",
    "        timesteps, dims = W.size()\n",
    "        for pos in range(timesteps):\n",
    "            for dim in range(dims):\n",
    "                val = torch.tensor( pos / 10000 ** (dim / dims) )\n",
    "                if dim % 2 == 0:\n",
    "                    W[pos, dim] = torch.sin(val)\n",
    "                else:\n",
    "                    W[pos, dim] = torch.cos(val)\n",
    "        self.embedding_layer.weight = torch.nn.Parameter(W, requires_grad=False)\n",
    "\n",
    "    def forward(self, X: torch.LongTensor) -> torch.FloatTensor:\n",
    "        return self.embedding_layer(X)\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self.embedding_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1610742910>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_emb = PositionalEmbedding(_MAXLEN, _HIDDEN_SIZE)\n",
    "plt.imshow(\n",
    "    pos_emb(torch.arange(0, 32)).cpu().numpy(),\n",
    "    cmap='Greys'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### The transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    \"\"\"Token and positional embedding for the Transformer.\"\"\"\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, maxlen: int) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.maxlen = maxlen\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.hidden_size,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        self.pos_embedding = PositionalEmbedding(\n",
    "            maxlen,\n",
    "            hidden_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, X: torch.LongTensor) -> torch.FloatTensor:\n",
    "        B, T = X.size()\n",
    "        w_emb = self.word_embedding(X)\n",
    "        p_emb = self.pos_embedding(self.get_position_ints(B, T))\n",
    "        # scale by sqrt(hidden size), Vaswani et al. Section 3.4\n",
    "        #emb = math.sqrt(self.hidden_size) * \n",
    "        emb = w_emb + p_emb\n",
    "        return emb\n",
    "    \n",
    "    def get_mask(self, X: torch.LongTensor) -> torch.ByteTensor:\n",
    "        \"\"\"Get the mask for the transformer.\"\"\"\n",
    "        return (X == 0)  # (B, T)\n",
    "    \n",
    "    def get_position_ints(self, batch_size: int, length: int) -> torch.LongTensor:        \n",
    "        X = torch.zeros(batch_size, length, dtype=torch.int64)\n",
    "        X[:] = torch.arange(length)\n",
    "        return X.to(self.device)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return list(self.parameters())[0].device\n",
    "\n",
    "\n",
    "class DateEncoder(nn.Module):\n",
    "    def __init__(self, tok_to_int) -> None:\n",
    "        super().__init__()\n",
    "        self.maxlen = _MAXLEN\n",
    "        self.vocab_size = len(tok_to_int)\n",
    "        self.hidden_size = _HIDDEN_SIZE\n",
    "        self.num_layers = _NUM_LAYERS\n",
    "        self.num_heads = _NUM_HEADS\n",
    "\n",
    "        self.embedding_layer = TransformerEmbedding(\n",
    "            self.vocab_size,\n",
    "            self.hidden_size,\n",
    "            self.maxlen\n",
    "        )\n",
    "\n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            self.hidden_size,\n",
    "            self.num_heads,\n",
    "            dim_feedforward=self.hidden_size * self.num_heads,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            transformer_layer,\n",
    "            self.num_layers,\n",
    "            norm=nn.LayerNorm(self.hidden_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X: torch.LongTensor) -> torch.FloatTensor:\n",
    "        emb = self.embedding_layer(X)  # (B, T, e)\n",
    "        emb_t = torch.transpose(emb, 0, 1)  # (T, B, e)\n",
    "        mask = self.embedding_layer.get_mask(X)  # (B, T)\n",
    "        emb_t = self.transformer(emb_t, src_key_padding_mask=mask)  # (T, B, e)\n",
    "        return emb_t\n",
    "\n",
    "\n",
    "class DateClassifier(nn.Module):\n",
    "    def __init__(self, tok_to_int: dict, output_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.encoder = DateEncoder(tok_to_int)\n",
    "        self.densor = nn.Sequential(\n",
    "            nn.Linear(self.encoder.hidden_size, self.encoder.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.encoder.hidden_size, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X: torch.LongTensor) -> torch.FloatTensor:\n",
    "        emb = self.encoder(X)  # (T, B, e)\n",
    "        last_emb = emb[-1]  # (B, e)\n",
    "        logit = self.densor(last_emb)\n",
    "        log_prob = F.log_softmax(logit, dim=-1)\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hooks let us inspect the inputs/outputs of intermediate layers\n",
    "\n",
    "For more examples and explanation.\n",
    "https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def decode_tensor(x: torch.LongTensor, int_to_tok: dict, strip_pad: bool = False):\n",
    "    \"\"\"Decode the model output, int -> char.\n",
    "    Optionally remove all padding characters.\n",
    "    \"\"\"\n",
    "    if strip_pad:\n",
    "        return [int_to_tok[i] for i in x if i!=0]\n",
    "    return [int_to_tok[i] for i in x]\n",
    "\n",
    "\n",
    "def find_attn_modules(model: nn.Module, attn_modules: list, depth = 0):\n",
    "    \"\"\"Get all the attention modules in a model.\"\"\"\n",
    "    for i, module in enumerate(model.children()):\n",
    "        if isinstance(module, nn.MultiheadAttention):\n",
    "            attn_modules += [module]\n",
    "        else:\n",
    "            find_attn_modules(module, attn_modules, depth + 1)\n",
    "\n",
    "\n",
    "class AttnModelHooks:\n",
    "    def __init__(self, model) -> None:\n",
    "        self.data = dict()\n",
    "        self.embedding_module = model.encoder.embedding_layer\n",
    "        self.output_module = model.densor\n",
    "        self.attn_modules = []\n",
    "        find_attn_modules(model, self.attn_modules, 0)\n",
    "        \n",
    "    def register(self):\n",
    "        self.embedding_module.register_forward_hook(self.text_hook)\n",
    "        self.output_module.register_forward_hook(self.output_hook)\n",
    "        for layer in self.attn_modules:\n",
    "            layer.register_forward_hook(self.attn_hook)\n",
    "\n",
    "    def text_hook(self, module, inputs, outputs):\n",
    "        assert isinstance(module, TransformerEmbedding)\n",
    "        print(type(inputs))\n",
    "        print(len(inputs))\n",
    "        token_ids = inputs[0]\n",
    "        decoded = [decode_tensor(ids, int_to_tok, strip_pad=True) for ids in token_ids]\n",
    "        self.data['text'] = decoded\n",
    "\n",
    "    def attn_hook(self, module, inputs, outputs):\n",
    "        assert isinstance(module, nn.MultiheadAttention)\n",
    "        attn_weights = outputs[-1]\n",
    "        self.data[module] = attn_weights\n",
    "    \n",
    "    def output_hook(self, module, inputs, outputs):\n",
    "        assert isinstance(module, nn.Sequential)\n",
    "        logit = outputs[0]\n",
    "        self.data['logit'] = logit\n",
    "\n",
    "    def get_attn_data(self, idx: int) -> torch.FloatTensor:\n",
    "        attn = []\n",
    "        for layer in self.attn_modules:\n",
    "            attn.append(self.data.get(layer)[idx])\n",
    "        return torch.stack(attn, dim=0)\n",
    "\n",
    "    def get_text_data(self, idx: int) -> List[str]:\n",
    "        return self.data['text'][idx]\n",
    "\n",
    "    def plot_attn(self, idx: int):\n",
    "        attn_weights = self.get_attn_data(idx)\n",
    "        text = self.get_text_data(idx)\n",
    "        N = len(text)\n",
    "        ticks = np.arange(N)\n",
    "        for dim in range(attn_weights.size(0)):\n",
    "            attn_vals = attn_weights[dim].detach().cpu().numpy()\n",
    "            attn_vals = attn_vals[-N:, -N:]\n",
    "            plt.imshow(attn_vals, cmap='gray', vmin=0., vmax=1.)\n",
    "            plt.yticks(ticks, text)\n",
    "            plt.xticks(ticks, text, rotation=45)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DateClassifier(tok_to_int, data.c)\n",
    "hooks = AttnModelHooks(model)\n",
    "learner = Learner(data, model, loss_func=nn.NLLLoss(), opt_func=optim.Adam, metrics=[accuracy])\n",
    "hooks.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='31' class='' max='390', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      7.95% [31/390 00:08<01:35 2.5871]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f1622122170>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/myen/miniconda3/envs/date/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/myen/miniconda3/envs/date/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/myen/miniconda3/envs/date/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/myen/miniconda3/envs/date/lib/python3.7/multiprocessing/popen_fork.py\", line 48, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/home/myen/miniconda3/envs/date/lib/python3.7/multiprocessing/popen_fork.py\", line 28, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/date/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/date/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_loss_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/date/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-85e320f93985>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (T, B, e)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mlast_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (B, e)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/date/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-85e320f93985>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, T, e)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0memb_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (T, B, e)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/date/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-cecf4e12c645>\u001b[0m in \u001b[0;36mtext_hook\u001b[0;34m(self, module, inputs, outputs)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdecode_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_to_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_pad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-cecf4e12c645>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdecode_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_to_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_pad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-cecf4e12c645>\u001b[0m in \u001b[0;36mdecode_tensor\u001b[0;34m(x, int_to_tok, strip_pad)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstrip_pad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint_to_tok\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint_to_tok\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-cecf4e12c645>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstrip_pad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint_to_tok\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint_to_tok\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-dfd2171f1eb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/date/lib/python3.7/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(learn, start_lr, end_lr, num_it, stop_div, wd)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLRFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_it\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m def to_fp16(learn:Learner, loss_scale:float=None, max_noskip:int=1000, dynamic:bool=True, clip:float=None,\n",
      "\u001b[0;32m~/miniconda3/envs/date/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/date/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m loss_func_name2activ = {'cross_entropy_loss': F.softmax, 'nll_loss': torch.exp, 'poisson_nll_loss': torch.exp,\n",
      "\u001b[0;32m~/miniconda3/envs/date/lib/python3.7/site-packages/fastai/callback.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, exception)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mException\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;34m\"Handle end of training, `exception` is an `Exception` or False if no exceptions during training.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/date/lib/python3.7/site-packages/fastai/callback.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, cb_name, call_mets, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcall_mets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/date/lib/python3.7/site-packages/fastai/callback.py\u001b[0m in \u001b[0;36m_call_and_update\u001b[0;34m(self, cb, cb_name, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;34m\"Call `cb_name` on `cb` and update the inner state.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'on_{cb_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/date/lib/python3.7/site-packages/fastai/callbacks/lr_finder.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m\"Cleanup learn model weights disturbed during LRFinder exploration.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tmp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpurge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/date/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, file, device, strict, with_opt, purge, remove_module)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34mf'{file}.pth'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_pathlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mdistrib_barrier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'opt'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mmodel_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/date/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'encoding'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/date/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learner.lr_find()\n",
    "learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.286405</td>\n",
       "      <td>0.219345</td>\n",
       "      <td>0.873300</td>\n",
       "      <td>01:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.210547</td>\n",
       "      <td>0.202374</td>\n",
       "      <td>0.897100</td>\n",
       "      <td>01:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(2, max_lr=1e-2)\n",
    "#learner.fit(5, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hooks.plot_attn(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
